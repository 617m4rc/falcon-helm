{{- if .Values.node.enabled }}
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {{ include "falcon-sensor.fullname" . }}
  labels:
    name: {{ include "falcon-sensor.fullname" . }}
    app: {{ include "falcon-sensor.fullname" . }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    heritage: {{ .Release.Service | quote }}
    release: {{ .Release.Name | quote }}
    {{- if .Values.node.daemonset.labels }}
    {{- range $key, $value := .Values.node.daemonset.labels }}
    {{ $key }}: {{ $value | quote }}
    {{- end }}
    {{- end }}
  {{- if .Values.node.daemonset.annotations }}
  annotations:
    {{- range $key, $value := .Values.node.daemonset.annotations }}
    {{ $key }}: {{ $value | quote }}
    {{- end }}
  {{- end }}
  namespace: {{ .Values.node.namespace }}
spec:
  selector:
    matchLabels:
      name: {{ include "falcon-sensor.fullname" . }}
      app: {{ include "falcon-sensor.fullname" . }}
      release: {{ .Release.Name | quote }}
  updateStrategy:
    type: {{ .Values.node.daemonset.updateStrategy }}
  template:
    metadata:
      annotations:
        {{- range $key, $value := .Values.node.podAnnotations }}
        {{ $key }}: {{ $value | quote }}
        {{- end }}
      labels:
        name: {{ include "falcon-sensor.fullname" . }}
        app: {{ include "falcon-sensor.fullname" . }}
        chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
        heritage: {{ .Release.Service | quote }}
        release: {{ .Release.Name | quote }}
        {{- if .Values.node.daemonset.labels }}
        {{- range $key, $value := .Values.node.daemonset.labels }}
        {{ $key }}: {{ $value | quote }}
        {{- end }}
        {{- end }}
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      initContainers:
      # This init container creates empty falconstore file so that when
      # it's mounted into the sensor-node-container, k8s would just use it
      # rather than creating a directory.  Mounting falconstore file as
      # a file volume ensures that AID is preserved across container
      # restarts.
      - name: init-falconstore
        image: busybox
        args: [/bin/sh, -c, 'touch /tmp/CrowdStrike/falconstore']
        volumeMounts:
          - name: falconstore-dir
            mountPath: /tmp/CrowdStrike
      containers:
      - name: sensor-node-container
        image: "{{ .Values.node.image.repository }}:{{ .Values.node.image.tag }}"
        imagePullPolicy: "{{ .Values.node.image.pullPolicy }}"
        volumeMounts:
        - name: dev
          mountPath: /dev
        - name: var-run
          mountPath: /var/run
        - name: etc
          mountPath: /etc
        - name: var-log
          mountPath: /var/log
        - name: falconstore
          mountPath: /opt/CrowdStrike/falconstore
        securityContext: {{ toYaml ( .Values.node.daemonset.securityContext ) | nindent 10 }}
        envFrom:
        - configMapRef:
            name: {{ include "falcon-sensor.fullname" . }}-config
      # This spits out logs from sensor-node-container to stdout so that they
      # are routed through k8s log driver.
      - name: log
        image: busybox
        args: [/bin/sh, -c, 'tail -n1 -f /var/log/falcon-sensor.log']
        volumeMounts:
        - name: var-log
          mountPath: /var/log
      volumes:
        - name: dev
          hostPath:
            path: /dev
        - name: etc
          hostPath:
            path: /etc
        - name: var-run
          hostPath:
            path: /var/run
        - name: var-log
          emptyDir: {}
        - name: falconstore
          hostPath:
            path: /tmp/CrowdStrike/falconstore
        - name: falconstore-dir
          hostPath:
            path: /tmp/CrowdStrike
      terminationGracePeriodSeconds: {{ .Values.node.terminationGracePeriod }}
      hostNetwork: true
      hostPID: true
      hostIPC: true
{{- end }}
